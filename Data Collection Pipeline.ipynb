{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4931a26-65d4-4f13-9525-94be9d770b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fox-trot\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Face Mesh solution\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to calculate the Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Threshold for detecting closed eyes\n",
    "EAR_THRESHOLD = 0.24\n",
    "CONSECUTIVE_FRAMES = 10\n",
    "\n",
    "# Create directories for saving images\n",
    "base_dir = 'dataset'\n",
    "drowsy_dir = os.path.join(base_dir, 'drowsy')\n",
    "alert_dir = os.path.join(base_dir, 'alert')\n",
    "os.makedirs(drowsy_dir, exist_ok=True)\n",
    "os.makedirs(alert_dir, exist_ok=True)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for tracking eye closure\n",
    "consecutive_frames_blinking = 0\n",
    "eye_closed = False\n",
    "image_count_drowsy = 0\n",
    "image_count_alert = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to get facial landmarks\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Extract landmarks for the left and right eye\n",
    "            left_eye = np.array([[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] \n",
    "                                for i in [33, 160, 158, 133, 153, 144]])\n",
    "            right_eye = np.array([[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] \n",
    "                                 for i in [362, 385, 387, 263, 373, 380]])\n",
    "\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            h, w, _ = frame.shape\n",
    "            left_eye = (left_eye * [w, h]).astype(int)\n",
    "            right_eye = (right_eye * [w, h]).astype(int)\n",
    "\n",
    "            # Calculate EAR for both eyes\n",
    "            left_ear = calculate_ear(left_eye)\n",
    "            right_ear = calculate_ear(right_eye)\n",
    "\n",
    "            # Average EAR\n",
    "            ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "            # Check if EAR is below the threshold\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                consecutive_frames_blinking += 1\n",
    "                if consecutive_frames_blinking >= CONSECUTIVE_FRAMES:\n",
    "                    eye_closed = True\n",
    "            else:\n",
    "                consecutive_frames_blinking = 0\n",
    "                eye_closed = False\n",
    "\n",
    "            # Display EAR and eye status\n",
    "            cv2.putText(frame, f'EAR: {ear:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if eye_closed:\n",
    "                cv2.putText(frame, 'Eyes Closed', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                image_path = os.path.join(drowsy_dir, f'drowsy_{image_count_drowsy}.jpg')\n",
    "                image_count_drowsy += 1\n",
    "            else:\n",
    "                image_path = os.path.join(alert_dir, f'alert_{image_count_alert}.jpg')\n",
    "                image_count_alert += 1\n",
    "\n",
    "            # Save image based on eye status\n",
    "            if eye_closed or ear > EAR_THRESHOLD:\n",
    "                cv2.imwrite(image_path, frame)\n",
    "\n",
    "            # Draw the eyes\n",
    "            for (x, y) in left_eye:\n",
    "                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "            for (x, y) in right_eye:\n",
    "                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0511af92-fe37-45a3-b1fb-2e91e25d4852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fox-trot\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Initialize MediaPipe Face Mesh solution\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to calculate the Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Threshold for detecting closed eyes\n",
    "EAR_THRESHOLD = 0.24\n",
    "CONSECUTIVE_FRAMES = 10\n",
    "IMAGE_CAPTURE_RATE = 4 \n",
    "\n",
    "x1, y1 = 150, 150\n",
    "x2, y2 = 550, 550\n",
    "\n",
    "base_dir = 'dataset'\n",
    "drowsy_dir = os.path.join(base_dir, 'drowsy')\n",
    "alert_dir = os.path.join(base_dir, 'alert')\n",
    "os.makedirs(drowsy_dir, exist_ok=True)\n",
    "os.makedirs(alert_dir, exist_ok=True)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for tracking eye closure\n",
    "consecutive_frames_blinking = 0\n",
    "eye_closed = False\n",
    "\n",
    "# Time interval between captures\n",
    "time_interval = 1.0 / IMAGE_CAPTURE_RATE\n",
    "last_capture_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to get facial landmarks\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Extract landmarks for the left and right eye\n",
    "            left_eye = np.array([[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] \n",
    "                                for i in [33, 160, 158, 133, 153, 144]])\n",
    "            right_eye = np.array([[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] \n",
    "                                 for i in [362, 385, 387, 263, 373, 380]])\n",
    "\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            h, w, _ = frame.shape\n",
    "            left_eye = (left_eye * [w, h]).astype(int)\n",
    "            right_eye = (right_eye * [w, h]).astype(int)\n",
    "\n",
    "            # Calculate EAR for both eyes\n",
    "            left_ear = calculate_ear(left_eye)\n",
    "            right_ear = calculate_ear(right_eye)\n",
    "\n",
    "            # Average EAR\n",
    "            ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "            # Check if EAR is below the threshold\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                consecutive_frames_blinking += 1\n",
    "                if consecutive_frames_blinking >= CONSECUTIVE_FRAMES:\n",
    "                    eye_closed = True\n",
    "            else:\n",
    "                consecutive_frames_blinking = 0\n",
    "                eye_closed = False\n",
    "\n",
    "            # Crop the frame using specified coordinates\n",
    "            cropped_frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Save image and EAR value based on eye status if it's time to capture\n",
    "            current_time = time.time()\n",
    "            if current_time - last_capture_time >= time_interval:\n",
    "                last_capture_time = current_time\n",
    "\n",
    "                # Generate a unique directory for each capture\n",
    "                uuid_str = str(uuid.uuid4())\n",
    "                if eye_closed:\n",
    "                    directory = os.path.join(drowsy_dir, uuid_str)\n",
    "                else:\n",
    "                    directory = os.path.join(alert_dir, uuid_str)\n",
    "                \n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "                # Save the image\n",
    "                image_path = os.path.join(directory, 'image.jpg')\n",
    "                cv2.imwrite(image_path, cropped_frame)\n",
    "\n",
    "                # Save the EAR value to a text file\n",
    "                ear_path = os.path.join(directory, 'EAR.txt')\n",
    "                with open(ear_path, 'w') as f:\n",
    "                    f.write(f'{ear:.2f}')\n",
    "\n",
    "    # Display the cropped frame (optional)\n",
    "    cv2.imshow('Frame', cropped_frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38b3bf-5713-4e09-bba8-35f9c7c9f275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
